<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="OpenSatMap">
  <meta name="keywords" content="OpenSatMap">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>OpenSatMap</title>

  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-YQCWF52Y54"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'G-YQCWF52Y54');
  </script>
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="stylesheet" href="./static/css/image-compare-viewer.min.css">
  <link rel="icon" href="./static/images/OIP.jpg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script src="./static/js/image-compare-viewer.min.js"></script>
  <!-- <style>
        #toc {
            border: 1px solid #ccc;
            padding: 10px;
            margin-bottom: 20px;
            max-width: 250px; /* Ë∞ÉÊï¥ÁõÆÂΩïÂÆπÂô®ÁöÑÂÆΩÂ∫¶ */
        }
        #toc ul {
            list-style-type: none;
            padding-left: 0;
        }
        #toc ul li {
            margin-bottom: 5px;
        }
        #toc ul li a {
            text-decoration: none;
            color: blue;
        }
  </style> -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>

  <!-- and it's easy to individually load additional languages -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/go.min.js"></script>

  <script>hljs.highlightAll();</script>
</head>

<body>


  <!-- <section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-2 publication-title"></h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              Anonymous Authors
          </div>
        </div>
      </div>
    </div>
  </div>
</section> -->
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">OpenSatMap: A Fine-grained High-resolution Satellite Dataset for
              Large-scale Map Construction</h1>
            <!-- <h2 class="title is-3">CVPR 2024</h2> -->
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://bjzhb666.github.io/">Hongbo Zhao</a>*<sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://lue.fan/">Lue Fan</a>*<sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=iLOoUqIAAAAJ">Yuntao Chen</a>*<sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://haochen-wang409.github.io/">Haochen Wang</a>*<sup>1</sup>,
              </span>
              <span class="author-block">
                <a>Yuran Yang</a>*<sup>3</sup>,
              </span>
              <span class="author-block">
                <a>Xiaojuan Jin</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a>Yixin Zhang</a><sup>3</sup>,
              </span>
              <span class="author-block">
                <a href="https://people.ucas.edu.cn/~gfmeng">Gaofeng Meng</a><sup>1,2</sup>,
              </span>
              <span class="author-block">
                <a href="https://zhaoxiangzhang.net/">Zhaoxiang Zhang</a><sup>1,2</sup>,
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>CASIA &amp; UCAS,</span>
              <span class="author-block"><sup>2</sup>CAIR, HKISI, CAS,</span>
              <span class="author-block"><sup>3</sup>Tencent Maps, Tencent</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <span class="link-block">
                  <a href="http://arxiv.org/abs/2410.23278" class="external-link button is-normal is-rounded is-dark"> <!-- TODO: update the link -->
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="http://arxiv.org/abs/2410.23278" class="external-link button is-normal is-rounded is-dark">
                    <!-- TODO: update the link -->
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://colab.research.google.com/github/bjzhb666/OpensatMap-demo" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <img src="https://main-educational.github.io/_images/logo_colab.png">
                    </span>
                    <span>Dataset Demo</span>
                  </a>
                </span>
                <!-- Video Link. -->
                <!-- <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
                <!-- Code Link. -->
                <span class="link-block">
                  <a href="https://github.com/bjzhb666/get_google_maps_image" class="external-link button is-normal is-rounded is-dark"> <!-- TODO: update the link -->
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
                <!-- Dataset Link. -->
                <span class="link-block">
                  <a href="https://huggingface.co/datasets/z-hb/OpenSatMap/tree/main"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="far fa-images"></i>
                    </span>
                    <span>Data</span>
                  </a>
                </span>
              </div>

            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section>
    <div class="hero-body">
      <div class="container is-max-desktop">
        <!-- Abstract. -->
        <div class="columns is-centered has-text-centered">
          <div class="column is-full-width is-centered">
            <div class="content has-text-justified">
              <img src="./static/images/figure1_black.jpg" width="100%">
              <p>
                <b>Demonstrations of OpenSatMap dataset.</b>
                It contains high-resolution satellite images with fine-grained annotations, covering diverse geographic
                locations and popular driving datasets.
              </p>
            </div>
          </div>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </section>


  <!-- <div id="toc"></div>
  <ul>
    TOC will be generated here -->
  <!-- </ul> -->

  <section>
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths is-centered">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <!-- <div class="content has-text-justified">
            <img src="./static/images/WX20231125-133933@2x.png" width="100%"> -->
            <!-- <p>
            <b>Multiview visual forecasting and planning by world model.</b>
            At time step \( T \), the world model imagines the multiple futures at \(T+K\), and finds it is safe to keep going straight at \(T\). Then the model realizes that the ego car will be too close to the front car according to the imagination of time step \(T + 2K\), so it decides to change to the left lane for a safe overtaking.
          </p> -->
            <!-- </div> -->
            <p>
              In this paper, we propose <b>OpenSatMap</b>, a fine-grained, high-resolution satellite dataset for
              large-scale map construction.
              Map construction is one of the foundations of the transportation industry, such as navigation and
              autonomous driving.
              Extracting road structures from satellite images is an efficient way to construct large-scale maps.
              However, existing satellite datasets provide only coarse semantic-level labels with a relatively low
              resolution (up to level 19), impeding the advancement of this field.
              In contrast, the proposed <b>OpenSatMap</b> (1) has fine-grained instance-level annotations; (2) consists
              of high-resolution images (level 20); (3) is currently the largest one of its kind; (4) collects data with
              high diversity.
              Moreover, <b>OpenSatMap</b> covers and aligns with the popular nuScenes dataset and Argoverse 2 dataset to
              potentially advance autonomous driving technologies.
              By publishing and maintaining the dataset, we provide a high-quality benchmark for satellite-based map
              construction and downstream tasks like autonomous driving.
            </p>
          </div>
        </div>
      </div>
      <!--/ Abstract. -->
    </div>
  </section>

  <!-- <section  class="section">
  <div class="container is-max-desktop has-text-centered"> 
    <h2 class="title is-3">Introduction Video</h2>
      <div class="columns is-centered">
        <div class="column is-full-width">
          <video poster="" id="intro" autoplay controls muted loop playsinline width="100%">
          <source src="" type="video/mp4">
        </video>
        </div>
     </div>
  </div>
</section> -->

  <section class="section">
    <div class="container is-max-desktop has-text-centered">
      <h2 class="title is-3">News</h2>
      <div class="column is-full-width">
        <div class="content has-text-justified">
          <!-- iterize -->
          <li>üí• [Oct, 2024]
            <b>OpenSatMap dataset is released!</b>
            We are excited to announce the release of OpenSatMap dataset, a fine-grained high-resolution satellite
            dataset for large-scale map construction.
            The dataset contains high-resolution satellite images with fine-grained annotations, covering diverse
            geographic locations and popular driving datasets.
            The dataset is available for download <a href="https://huggingface.co/datasets/z-hb/OpenSatMap/tree/main">here</a>.
            <!-- <b><span style="color:red;">New</span></b>  -->
            <strong><a href="https://zhaoxiangzhang.net/wp-content/uploads/2020/02/new.gif"><img decoding="async"
                  class="alignnone size-full wp-image-105"
                  src="https://zhaoxiangzhang.net/wp-content/uploads/2020/02/new.gif" alt="new" width="29"
                  height="13" /></a></strong>
          </li>
          <!-- <li>
            <a href="dataset.html">Dataset</a>
          </li>
          <li>
            <a href="code.html">Code</a>
          </li>
          <li>
            <a href="tasks.html">Tasks</a>
          </li>
          <li>
            <a href="evaluation.html">Evaluation</a>
          </li> -->
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop has-text-centered">
      <h2 class="title is-3">OpenSatMap Dataset</h2>
      <div class="columns is-centered">
        <div class="column">
          <div class="content">
            <div class="content has-text-justified">
              <h3 class="title is-4"> Description</h3>
              The dataset contains 3,787 high-resolution satellite images with fine-grained annotations, covering
              diverse geographic locations and popular driving datasets.
              It can be used for large-scale map construction and downstream tasks like autonomous driving.
              The images are collected from Google Maps at level 19 resolution (0.3m/pixel) and level 20 resolution
              (0.15m/pixel), we denote them as OpenSatMap19 and OpenSatMap20, respectively.
              For OpenSatMap19, the images are collected from 8 cities in China, including Beijing, Shanghai, Guangzhou,
              ShenZhen, Chengdu, Xi'an, Tianjin, and Shenyang. There are 1806 images in OpenSatMap19.
              For OpenSatMap20, the images are collected from 18 countries, more than 50 cities all over the world.
              There are 1981 images in OpenSatMap20.
              The figure below shows the sampling areas of the images in OpenSatMap.
              <!-- ÊèíÂÖ•html -->
              <p>
            <div class="columns is-centered">
              <iframe src="static/gps_points.html" width="800" height="550"></iframe>
            </div>
            </p>
          
                For each image, we provide instance-level annotations and eight attributes for road structures,
                including
                lanes lines, curb and
                virtual lines.
                The instances in OpenSatMap images are annotated by experts in remote sensing and computer vision.
                We will continue to update the dataset, to grow in size and scope to reflect evolving real-world
                conditions.
         
              <h3 class="title is-4">Examples of Annotated Images</h3>

                These are two examples of annotated images from OpenSatMap20. 
                Or you can play it in the
                <a href="https://colab.research.google.com/github/bjzhb666/OpensatMap-demo">
                  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"></a>
                <!-- <a href="https://colab.research.google.com/github/bjzhb666/OpensatMap-demo">demo notebook</a>   -->
                to visualize the annotations by yourself.
     
              <div class="column">
                <div class="content">
                  <div class="columns is-centered">
                    <div class="column is-full-width">

                      <div class="image-compare shadow-md rounded mb-16">
                        <img src="static/images/singapore-onenorth_-1_-1_satvisual-ori.png" alt="" />
                        <img src="static/images/singapore-onenorth_-1_-1_satvisual-instance.png" alt="" />
                      </div>
                      <p>
                        <div class="container is-max-desktop has-text-centered">
                        <b>Example 1:</b> Annotated image from OpenSatMap20. The image is collected from Singapore Onenorth (nuScenes).
                      </div>
                      </p>
                      <div class="image-compare shadow-md rounded mb-16">
                        <img src="static/images/flyover2_0_0_sat.png" alt="" />
                        <img src="static/images/flyover2_0_0_satvisual-instance.png" alt="" />
                      </div>
                      <p>
                        <div class="container is-max-desktop has-text-centered">
                        <b>Example 2:</b> Annotated image from OpenSatMap20. The image contains a flyover and roundabout, which is complex.
                      </div>
                      </p>

                    </div>
                  </div>
                </div>
              </div>

              <h3 class="title is-4">Image Source and Usage License</h3>
              The OpenSatMap images are collected from Google Maps.
              The dataset will be licensed under a Creative Commons CC-BY-NC-SA 4.0 license
              and the usage of the images must respect the
              <a href="https://about.google/brand-resource-center/products-and-services/geo-guidelines/">Google Maps
                Terms of Service</a>.
              <h3 class="title is-4">Line Category and Attribute</h3>
              We use vectorized polylines to represent a line instance.
              We first categorize all lines into three categories: <b>curb, lane line</b>, and <b>virtual line</b>.
              A curb is the boundary of a road. Lane lines are those visible lines forming the lanes. A virtual line
              means that there is no lane line or curb here, but logically there should be a boundary to form a full
              lane. Please refer to the figure below for examples of these three categories.
              <!-- insert an image -->
              <p>
              <div class="columns is-centered has-text-centered">
                <img src="./static/images/category.jpg" width="80%">
              </div>
              </p>
              For each line instance, we provide eight attributes: <b>color, line type,number of lines, function,
                bidirection, boundary, shaded, clearness</b>.
              Specifically, they are:
              <ul>
                <li><b>Color</b>: The color of the line. It can be white, yellow, others or none.</li>
                <li><b>Line type</b>: The type of the line. It can be solid, thick solid, dashed, short dashed dotted,
                  others or none.</li>
                <li><b>Number of lines</b>: The number of the line. It can be single, double, others or none.</li>
                <li><b>Function</b>: The function of the line. It can be Chevron markings, no parking, deceleration
                  line, bus lane, tidal line,
                  parking space, vehicle staging area, guide line, changable line, lane-borrowing line, others or none.
                </li>
                <li><b>Bidirection</b>: Whether the line is bidirectional. It can be true or false.</li>
                <li><b>Boundary</b>: Whether the line is a boundary. It can be true or false.</li>
                <li><b>Shaded</b>: The degree of occlusion. It can be no, minor or major.</li>
                <li><b>Clearness</b>: The clearness of the line. It can be clear or fuzzy.</li>
              </ul>
              Note that there is no man-made visible line on curbs and virtual lines, so we annotate their colors,
              line types, numbers of lines, and functions as <i>none</i>.
              <h3 class="title is-4">Annotation Format</h3>
              The annotations are stored in JSON format. Each image is annotated with "image_width", "image_height", and
              a list of "lines" where the elements are line instances.
              Each line is annotated with "category", "points", "color", "line_type", "line_num", "function",
              "bidirection", "boundary", "shaded", and "clearness".
              <pre><code class="language-python">{"img_name": {
    "image_width": int,
    "image_height": int,
    "lines": [
        {
            "category": str,
            "points": [
                [float, float],
                [float, float],
                [float, float], 
                ...
            ],
            "color": str,
            "line_type": str,
            "line_num": str,
            "function": str,
            "bidirection": bool,
            "boundary": bool,
            "shaded": str,
            "clearness": bool
        },
        {
            "category": str,
            "points": [
                [float, float],
                [float, float],
                [float, float], 
                ...
            ],
            "color": str,
            "line_type": str,
            "line_num": str,
            "function": str,
            "bidirection": bool,
            "boundary": bool,
            "shaded": str,
            "clearness": bool
        },
        ...
        ]
  }
}</code></pre>
              <h3 class="title is-4">Meta data</h3>
              The meta data of GPS coordinates and image acquisition time are also provided. The meta data is stored in
              a JSON file.
              <pre><code class="language-python">{
    "img_name": [
      {
        "centerGPS": [float, float],
        "centerWorld": [float, float],
        "filename": str
      },
      {
        "centerGPS": [float, float],
        "centerWorld": [float, float],
        "filename": str
      },
      ...
    ]
    ...
  }</code></pre>
              <h3 class="title is-4"> Download</h3>
              <!-- OpenSatMap mini: <a href="https://drive.google.com/drive/folders/1zKs-GV2aNobLDl6E17hTqy2L_WpNPkSb?usp=sharing">Google Drive</a>.
              , <a href="">Baidu Drive</a>, <a href="">Tencent Drive</a>  -->
                OpenSatMap full dataset: <a href="https://pan.baidu.com/s/1rmOV2__R99eOoW0rGJtNag?pwd=otg0">Baidu Drive</a>, 
                <a href="https://huggingface.co/datasets/z-hb/OpenSatMap/tree/main">Hugging Face</a>
              </p>
              <h3 class="title is-4">Task</h3>
              <h3 class="title is-5">Overview</h3>
              We introduce two tasks using OpenSatMap. Task 1 is instance-level line detection from satellite images.
              Task 2 is satellite-enhanced online map construction for autonomous driving.
              We are welcome to any other tasks using OpenSatMap. Please contact us for more information.
              <h3 class="title is-5">Task 1: Instance-level Line Detection</h3>
              The aim of this task is to extract road structures from satellite images at the instance level.
              For each instance, we use polylines as the vectorized representation and pixel-level masks as the
              rasterized representation.
              <h3 class="title is-5">Task 2: Satellite-enhanced Online Map Construction</h3>
              We use satellite images to enhance online map construction for autonomous driving.
              Inputs are carema images of an autonomous vehicle and satellite images of the same area and outputs are
              vectorized map elements around the vehicle.
              <h3 class="title is-4">Code </h3>
              <li>
                <a href="https://github.com/bjzhb666/get_google_maps_image">Here</a> is our code for data acquisition. The code is based on the <a
                  href="https://github.com/mitroadmaps/roadtracer/tree/master/dataset">RoadTracer</a>. <br>
              </li>
              <li>
                <a href=""> Here</a> is our code for instance-level line detection. The code is based on the <a
                  href="https://github.com/open-mmlab/mmsegmentation/">MMsegmentation</a>. <br>
              </li>
              <li>
                <a href=""> S-HDmap</a> is our code for map construction. The code is based on the <a
                  href="https://github.com/xjtu-cs-gao/SatforHDMap">SatforHDMap</a>.
              </li>

              <h3 class="title is-4">Contact</h3>
              For any questions, please contact <a href="mailto:zhaohongbo2022@ia.ac.cn">Hongbo Zhao</a> and <a
                href="mailto:fanlue2019@ia.ac.cn">Lue Fan</a>.
              Or you can open an issue in the <a href="https://github.com/OpenSatMap/OpenSatMap.github.io">OpenSatMap
                GitHub repository</a>.
              <h3 class="title is-4">Acknowledgement</h3>
              Our baseline codes are based on the <a
                href="https://github.com/open-mmlab/mmsegmentation/">MMsegmentation</a> and <a
                href="https://github.com/xjtu-cs-gao/SatforHDMap">SatforHDMap</a>.
              Our data acquisition code is based on the <a
                href="https://github.com/mitroadmaps/roadtracer/tree/master/dataset">RoadTracer</a>.
              We thank the authors for their contributions. We thank the anonymous reviewers for their valuable comments and 
              Ziguang Niu, Xinyu Gao for their support.
              <!-- <h3 class="title is-4">Contact</h3> -->
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              <script type="text/javascript" src="https://www.free-counters.org/count/fpyo"></script><br>
              This website template is borrowed from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>
              under the <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>
  <script>
    document.addEventListener("DOMContentLoaded", function () {
      // Ê†áÈ¢òÁº©ÂÜôÊò†Â∞ÑË°®
      const titleMapping = {
        "OpenSatMap: A Fine-grained High-resolution Satellite Dataset for Large-scale Map Construction": "OpenSatMap Dataset",
        "Abstract": "Abstract",
        "News": "News",
        "Generation of Diverse Multiview Videos": "Multiview Videos",
        "Generation of Multiple Plausible Futures": "Plausible Futures",
        "Scene 1 (Turn left)": "Scene 1 (L)",
        "Scene 1 (Turn right)": "Scene 1 (R)",
        "Scene 2 (Turn left)": "Scene 2 (L)",
        "Scene 2 (Go straight)": "Scene 2 (S)",
        "Scene 3 (Turn left)": "Scene 3 (L)",
        "Scene 3 (Go straight)": "Scene 3 (S)",
        "Visual Element Control": "Visual Control",
        "Light Changes": "Light Changes",
        "Weather Changes": "Weather Changes",
        "Control Foreground Object Layouts": "Foreground Control",
        "Action Condition": "Action Condition",
        "Other Datasets": "Other Datasets",
        "World Model for End-to-End Planning": "World Model",
        "GPT-4V as Reward Function for Non-vectorized Observations": "GPT-4V Reward Function"
        // Ê∑ªÂä†Êõ¥Â§öÁöÑÊò†Â∞Ñ
      };

      // Ëé∑ÂèñÊâÄÊúâÁöÑÊ†áÈ¢ò
      const headers = document.querySelectorAll("h1, h2, h3, h4, h5, h6");
      const toc = document.getElementById("toc").querySelector('ul');

      let tocHTML = '';

      headers.forEach(header => {
        // ‰∏∫ÊØè‰∏™Ê†áÈ¢òÂàõÂª∫‰∏Ä‰∏™ID
        const id = header.textContent.toLowerCase().replace(/ /g, "-");
        header.id = id;
        tocHTML += `<li><a href="#${id}">${header.innerText}</a></li>`;
        // // ÂàõÂª∫ÁõÆÂΩïÈ°π
        // const li = document.createElement("li");
        // const a = document.createElement("a");
        // a.href = `#${id}`;

        // ‰ΩøÁî®Áº©ÂÜôÊ†áÈ¢ò
        const shortTitle = titleMapping[header.textContent] || header.textContent;
        a.textContent = shortTitle;

        // Ê†πÊçÆÊ†áÈ¢òÁ∫ßÂà´ËÆæÁΩÆÁº©Ëøõ
        li.style.marginLeft = `${(parseInt(header.tagName[1]) - 1) * 20}px`;

        li.appendChild(a);
        ul.appendChild(li);
        toc.innerHTML = tocHTML;
      });

      // toc.appendChild(ul);
    });
  </script>

  <script>
    let imageCompareViewers = document.querySelectorAll(".image-compare");
    let configs = [
      {
        addCircle: true,
        verticalMode: false,
        hoverStart: true,
        showLabels: true,
        labelOptions: {
          before: 'Without label',
          after: 'With label',
          onHover: false
        },
      },
      {
        addCircle: true,
        verticalMode: false,
        hoverStart: true,
        showLabels: true,
        labelOptions: {
          before: 'Without label',
          after: 'With label',
          onHover: false
        },
      },
      // { 
      //   addCircle: true, 
      //   verticalMode: false, 
      //   hoverStart: true,
      //   showLabels: true,
      //   labelOptions: {
      //     before: 'GPNeRF [Yuqi 2023]',  
      //     after: 'Ours',
      //     onHover: false
      //   },
      // },
      // { 
      //   addCircle: true, 
      //   verticalMode: false, 
      //   hoverStart: true,
      //   showLabels: true,
      //   labelOptions: {
      //     before: '3DGS [Bernhard 2023]',  
      //     after: 'Ours',
      //     onHover: false
      //   },
      // }
    ];

    imageCompareViewers.forEach((element, i) => {
      new ImageCompare(element, configs[i]).mount();
    });
  </script>
</body>

</html>